{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20dfd143-5d22-4d6f-b949-990fb4210ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162962c4-51fb-4b2a-bacf-620f057909bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc_to_sdg_mapping = {\n",
    "    \"A01B\": [2],\n",
    "    \"A01C\": [2],\n",
    "    \"A61K\": [3],\n",
    "    \"A61P\": [3],\n",
    "    \"B01D\": [6, 12],\n",
    "    \"B60L\": [11, 13],\n",
    "    \"C02F\": [6],\n",
    "    \"C07D\": [3],\n",
    "    \"C12N\": [3, 9, 15],\n",
    "    \"E03B\": [6],\n",
    "    \"F03D\": [7, 13],\n",
    "    \"G06F\": [9],\n",
    "    \"G01N\": [3, 9],\n",
    "    \"H01L\": [7],\n",
    "    \"Y02A\": [6, 13],\n",
    "    \"Y02E\": [7, 13],\n",
    "    \"Y02T\": [11, 13],\n",
    "    \"Y02W\": [11, 12, 13],\n",
    "    \"B82Y\": [3, 9],\n",
    "    \"C10G\": [7, 12],\n",
    "    \"A23L\": [2, 3],\n",
    "    \"F24J\": [7],\n",
    "    \"C01B\": [6, 12],\n",
    "    \"E21B\": [7, 12],\n",
    "    \"F01K\": [7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21495be5-8332-43f2-ac2e-7f7803b7a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files():\n",
    "    # Set up directories\n",
    "    input_dir = Path(\"/home/jovyan/epo-sdg/epo_chunks\")\n",
    "    output_dir = Path(\"/home/jovyan/epo-sdg/output\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all .csv.gz files\n",
    "    gz_files = list(input_dir.glob(\"*.csv.gz\"))\n",
    "    print(f\"Found {len(gz_files)} .csv.gz files to process\")\n",
    "    \n",
    "    # Define output file\n",
    "    output_file = output_dir / \"sdg_mapped_patents.csv\"\n",
    "    \n",
    "    # Process each file and append results incrementally\n",
    "    total_matched_rows = 0\n",
    "    first_file = True\n",
    "    \n",
    "    for i, gz_file in enumerate(gz_files):\n",
    "        print(f\"Processing file {i+1}/{len(gz_files)}: {gz_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read compressed CSV file\n",
    "            with gzip.open(gz_file, 'rt') as f:\n",
    "                df = pd.read_csv(f)\n",
    "            \n",
    "            # Process the IPC column and find matches\n",
    "            matched_rows = process_ipc_codes(df)\n",
    "            \n",
    "            if len(matched_rows) > 0:\n",
    "                # Write to output file (with headers only for first file)\n",
    "                matched_rows.to_csv(\n",
    "                    output_file, \n",
    "                    mode='w' if first_file else 'a',\n",
    "                    header=first_file,\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Update counters\n",
    "                total_matched_rows += len(matched_rows)\n",
    "                print(f\"  Found {len(matched_rows)} matching rows, total: {total_matched_rows}\")\n",
    "                \n",
    "                # Update first_file flag\n",
    "                if first_file:\n",
    "                    first_file = False\n",
    "            else:\n",
    "                print(f\"  No matching rows found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {gz_file.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Print final statistics\n",
    "    if total_matched_rows > 0:\n",
    "        print(f\"\\nProcessing complete. Saved {total_matched_rows} rows to {output_file}\")\n",
    "        \n",
    "        # Read back the complete file for summary statistics\n",
    "        try:\n",
    "            final_df = pd.read_csv(output_file)\n",
    "            print_summary(final_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary: {str(e)}\")\n",
    "    else:\n",
    "        print(\"\\nNo matching patents found across all files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd8f9f8-9c71-44f2-beed-4381292e2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ipc_codes(df):\n",
    "    \"\"\"Process a dataframe and find rows with matching IPC codes\"\"\"\n",
    "    matched_rows = []\n",
    "    \n",
    "    # Check if 'ipc' column exists\n",
    "    if 'ipc' not in df.columns:\n",
    "        print(\"  Warning: 'ipc' column not found in dataframe\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        ipc_codes = str(row['ipc']).split(',') if pd.notna(row['ipc']) else []\n",
    "        \n",
    "        # Clean up IPC codes (remove whitespace)\n",
    "        ipc_codes = [code.strip() for code in ipc_codes]\n",
    "        \n",
    "        # Check for matches with our mapping\n",
    "        matched_sdgs = set()\n",
    "        matched_ipc_codes = []\n",
    "        matched_details = []  # To store the full details of matched codes\n",
    "        \n",
    "        for full_ipc_code in ipc_codes:\n",
    "            # Extract the main IPC code (first 4 characters) for matching\n",
    "            # Most IPC codes have format like A61K31/4745 where A61K is the main category\n",
    "            main_ipc_code = None\n",
    "            \n",
    "            if len(full_ipc_code) >= 4:\n",
    "                # Extract first 4 characters (class + subclass)\n",
    "                main_ipc_code = full_ipc_code[:4]\n",
    "            \n",
    "            if main_ipc_code in ipc_to_sdg_mapping:\n",
    "                matched_sdgs.update(ipc_to_sdg_mapping[main_ipc_code])\n",
    "                matched_ipc_codes.append(main_ipc_code)\n",
    "                matched_details.append(full_ipc_code)  # Store the full IPC code for reference\n",
    "        \n",
    "        # If we found any matches, create a new row\n",
    "        if matched_sdgs:\n",
    "            new_row = row.copy()\n",
    "            \n",
    "            # Add SDG binary columns\n",
    "            for sdg_num in range(18):  # 0 to 17\n",
    "                if sdg_num == 0:\n",
    "                    # SDG_0 is 1 if no SDGs are assigned (shouldn't happen in our case)\n",
    "                    new_row[f'sdg_{sdg_num}'] = 1 if len(matched_sdgs) == 0 else 0\n",
    "                else:\n",
    "                    # SDG_1 to SDG_17\n",
    "                    new_row[f'sdg_{sdg_num}'] = 1 if sdg_num in matched_sdgs else 0\n",
    "            \n",
    "            # Add additional useful columns\n",
    "            new_row['matched_ipc_codes'] = ','.join(matched_ipc_codes)\n",
    "            new_row['matched_full_codes'] = ','.join(matched_details)\n",
    "            new_row['assigned_sdgs'] = ','.join(map(str, sorted(matched_sdgs)))\n",
    "            \n",
    "            matched_rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(matched_rows) if matched_rows else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f019b2e-f4fd-44b6-8219-f3db3c19fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(df):\n",
    "    \"\"\"Print a summary of the processed data\"\"\"\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Total patents processed: {len(df)}\")\n",
    "    \n",
    "    # Count patents per SDG\n",
    "    print(\"\\nPatents per SDG:\")\n",
    "    for sdg_num in range(1, 18):\n",
    "        count = df[f'sdg_{sdg_num}'].sum()\n",
    "        print(f\"  SDG {sdg_num}: {count} patents\")\n",
    "    \n",
    "    # Find patents with multiple SDGs\n",
    "    sdg_columns = [f'sdg_{i}' for i in range(1, 18)]\n",
    "    df['total_sdgs'] = df[sdg_columns].sum(axis=1)\n",
    "    multi_sdg_patents = (df['total_sdgs'] > 1).sum()\n",
    "    print(f\"\\nPatents assigned to multiple SDGs: {multi_sdg_patents}\")\n",
    "    \n",
    "    # Most common IPC main categories\n",
    "    print(\"\\nMost common matched IPC main categories:\")\n",
    "    all_matched_codes = []\n",
    "    for codes in df['matched_ipc_codes']:\n",
    "        all_matched_codes.extend(codes.split(','))\n",
    "    \n",
    "    from collections import Counter\n",
    "    code_counts = Counter(all_matched_codes)\n",
    "    for code, count in code_counts.most_common(10):\n",
    "        print(f\"  {code}: {count} occurrences\")\n",
    "        \n",
    "    # Sample of detailed IPC codes\n",
    "    print(\"\\nSample of detailed IPC codes that were matched:\")\n",
    "    sample_details = []\n",
    "    for details in df['matched_full_codes'][:100]:  # Look at first 100 rows for sampling\n",
    "        sample_details.extend(details.split(','))\n",
    "    \n",
    "    sample_details = list(set(sample_details))  # Remove duplicates\n",
    "    if len(sample_details) > 15:\n",
    "        sample_details = sample_details[:15]  # Show at most 15 examples\n",
    "        \n",
    "    for code in sample_details:\n",
    "        print(f\"  {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8ae9e-495f-48eb-ba61-9b4b43538836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 .csv.gz files to process\n",
      "Processing file 1/49: epo_0033.csv.gz\n",
      "  Found 3258 matching rows, total: 3258\n",
      "Processing file 2/49: epo_0049.csv.gz\n",
      "  Found 2050 matching rows, total: 5308\n",
      "Processing file 3/49: epo_0007.csv.gz\n",
      "  Found 4975 matching rows, total: 10283\n",
      "Processing file 4/49: epo_0022.csv.gz\n",
      "  Found 4250 matching rows, total: 14533\n",
      "Processing file 5/49: epo_0003.csv.gz\n",
      "  Found 5413 matching rows, total: 19946\n",
      "Processing file 6/49: epo_0023.csv.gz\n",
      "  Found 4204 matching rows, total: 24150\n",
      "Processing file 7/49: epo_0017.csv.gz\n",
      "  Found 5871 matching rows, total: 30021\n",
      "Processing file 8/49: epo_0029.csv.gz\n",
      "  Found 3812 matching rows, total: 33833\n",
      "Processing file 9/49: epo_0046.csv.gz\n",
      "  Found 7317 matching rows, total: 41150\n",
      "Processing file 10/49: epo_0016.csv.gz\n",
      "  Found 3112 matching rows, total: 44262\n",
      "Processing file 11/49: epo_0047.csv.gz\n",
      "  Found 4984 matching rows, total: 49246\n",
      "Processing file 12/49: epo_0019.csv.gz\n",
      "  Found 2829 matching rows, total: 52075\n",
      "Processing file 13/49: epo_0018.csv.gz\n",
      "  Found 4154 matching rows, total: 56229\n",
      "Processing file 14/49: epo_0043.csv.gz\n",
      "  Found 5576 matching rows, total: 61805\n",
      "Processing file 15/49: epo_0030.csv.gz\n",
      "  Found 5758 matching rows, total: 67563\n",
      "Processing file 16/49: epo_0034.csv.gz\n",
      "  Found 5722 matching rows, total: 73285\n",
      "Processing file 17/49: epo_0037.csv.gz\n",
      "  Found 4652 matching rows, total: 77937\n",
      "Processing file 18/49: epo_0038.csv.gz\n",
      "  Found 6673 matching rows, total: 84610\n",
      "Processing file 19/49: epo_0044.csv.gz\n",
      "  Found 7633 matching rows, total: 92243\n",
      "Processing file 20/49: epo_0009.csv.gz\n",
      "  Found 4052 matching rows, total: 96295\n",
      "Processing file 21/49: epo_0045.csv.gz\n",
      "  Found 6108 matching rows, total: 102403\n",
      "Processing file 22/49: epo_0035.csv.gz\n",
      "  Found 4083 matching rows, total: 106486\n",
      "Processing file 23/49: epo_0002.csv.gz\n",
      "  Found 4120 matching rows, total: 110606\n",
      "Processing file 24/49: epo_0001.csv.gz\n",
      "  Found 5244 matching rows, total: 115850\n",
      "Processing file 25/49: epo_0028.csv.gz\n",
      "  Found 5364 matching rows, total: 121214\n",
      "Processing file 26/49: epo_0024.csv.gz\n",
      "  Found 5616 matching rows, total: 126830\n",
      "Processing file 27/49: epo_0036.csv.gz\n",
      "  Found 6000 matching rows, total: 132830\n",
      "Processing file 28/49: epo_0032.csv.gz\n",
      "  Found 5724 matching rows, total: 138554\n",
      "Processing file 29/49: epo_0004.csv.gz\n",
      "  Found 4555 matching rows, total: 143109\n",
      "Processing file 30/49: epo_0008.csv.gz\n",
      "  Found 4238 matching rows, total: 147347\n",
      "Processing file 31/49: epo_0013.csv.gz\n",
      "  Found 3577 matching rows, total: 150924\n",
      "Processing file 32/49: epo_0027.csv.gz\n",
      "  Found 3714 matching rows, total: 154638\n",
      "Processing file 33/49: epo_0021.csv.gz\n",
      "  Found 4262 matching rows, total: 158900\n",
      "Processing file 34/49: epo_0014.csv.gz\n",
      "  Found 5863 matching rows, total: 164763\n",
      "Processing file 35/49: epo_0006.csv.gz\n",
      "  Found 6310 matching rows, total: 171073\n",
      "Processing file 36/49: epo_0041.csv.gz\n",
      "  Found 5694 matching rows, total: 176767\n",
      "Processing file 37/49: epo_0011.csv.gz\n",
      "  Found 4315 matching rows, total: 181082\n",
      "Processing file 38/49: epo_0015.csv.gz\n",
      "  Found 3880 matching rows, total: 184962\n",
      "Processing file 39/49: epo_0031.csv.gz\n",
      "  Found 3999 matching rows, total: 188961\n",
      "Processing file 40/49: epo_0020.csv.gz\n",
      "  Found 5730 matching rows, total: 194691\n",
      "Processing file 41/49: epo_0039.csv.gz\n",
      "  Found 5051 matching rows, total: 199742\n",
      "Processing file 42/49: epo_0012.csv.gz\n",
      "  Found 3604 matching rows, total: 203346\n",
      "Processing file 43/49: epo_0025.csv.gz\n",
      "  Found 4382 matching rows, total: 207728\n",
      "Processing file 44/49: epo_0010.csv.gz\n",
      "  Found 5719 matching rows, total: 213447\n",
      "Processing file 45/49: epo_0048.csv.gz\n",
      "  Found 6894 matching rows, total: 220341\n",
      "Processing file 46/49: epo_0042.csv.gz\n",
      "  Found 7383 matching rows, total: 227724\n",
      "Processing file 47/49: epo_0040.csv.gz\n",
      "  Found 6700 matching rows, total: 234424\n",
      "Processing file 48/49: epo_0005.csv.gz\n",
      "  Found 3413 matching rows, total: 237837\n",
      "Processing file 49/49: epo_0026.csv.gz\n",
      "  Found 4309 matching rows, total: 242146\n",
      "\n",
      "Processing complete. Saved 242146 rows to /home/jovyan/epo-sdg/output/sdg_mapped_patents.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de16940b-324b-4c9b-a7fd-a443159089b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_266/663973732.py:4: DtypeWarning: Columns (25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/home/jovyan/epo-sdg/output/sdg_mapped_patents.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the big CSV\n",
    "df = pd.read_csv('/home/jovyan/epo-sdg/output/sdg_mapped_patents.csv')\n",
    "\n",
    "# Take a random sample of, say, 500 rows (without replacement)\n",
    "sampled = df.sample(n=250, random_state=42)\n",
    "\n",
    "# Write out the sample\n",
    "sampled.to_csv('/home/jovyan/epo-sdg/output/sample_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ee17c-030b-40d3-b41c-c236b21080de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Writing chunk #1 to /home/jovyan/epo-sdg/output/sdg_mapped_patents_0001.csv.gz (20000 rows)…\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "# ↑ Add this right after importing csv:\n",
    "csv.field_size_limit(sys.maxsize)  # bump the max field size to the platform limit\n",
    "\n",
    "def split_and_compress_csv(input_csv: Path, chunk_size: int = 20_000) -> bool:\n",
    "    \"\"\"\n",
    "    Splits the input CSV into chunks of `chunk_size` rows,\n",
    "    compresses each chunk to .csv.gz (keeping the header),\n",
    "    and deletes the original CSV when done.\n",
    "    \"\"\"\n",
    "    if not input_csv.exists():\n",
    "        print(f\"Error: Input file {input_csv} not found.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with input_csv.open('r', newline='') as f_in:\n",
    "            reader = csv.reader(f_in)\n",
    "            header = next(reader)\n",
    "            file_index = 1\n",
    "            buffer = []\n",
    "\n",
    "            for row in reader:\n",
    "                buffer.append(row)\n",
    "                if len(buffer) >= chunk_size:\n",
    "                    _write_chunk(buffer, header, input_csv.parent, input_csv.stem, file_index)\n",
    "                    file_index += 1\n",
    "                    buffer.clear()\n",
    "\n",
    "            # write any remaining rows\n",
    "            if buffer:\n",
    "                _write_chunk(buffer, header, input_csv.parent, input_csv.stem, file_index)\n",
    "\n",
    "        # delete the original large CSV\n",
    "        print(f\"Deleting original file {input_csv}...\")\n",
    "        input_csv.unlink()\n",
    "        print(\"All done.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during splitting/compression: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def _write_chunk(rows: list, header: list, out_dir: Path, stem: str, index: int):\n",
    "    \"\"\"\n",
    "    Write one chunk (including header) to a gzip-compressed CSV,\n",
    "    naming it like `{stem}_0001.csv.gz`.\n",
    "    \"\"\"\n",
    "    chunk_name = f\"{stem}_{index:04d}.csv.gz\"\n",
    "    chunk_path = out_dir / chunk_name\n",
    "    print(f\"  → Writing chunk #{index} to {chunk_path} ({len(rows)} rows)…\")\n",
    "\n",
    "    with gzip.open(chunk_path, 'wt', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    size_mb = chunk_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"    • Compressed size: {size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = Path(\"/home/jovyan/epo-sdg/output/sdg_mapped_patents.csv\")\n",
    "    split_and_compress_csv(input_path, chunk_size=20_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e1558-7e10-4c9d-9e7e-5298107457ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
